{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import compiler_gym\n",
    "import ray, ray.tune as tune\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from compiler_gym.wrappers import ConstrainedCommandline, TimeLimit, CycleOverBenchmarks\n",
    "from itertools import islice\n",
    "from matplotlib import pyplot as plt\n",
    "from compiler_gym.envs import LlvmEnv\n",
    "\n",
    "print(\"compiler_gym version:\", compiler_gym.__version__)\n",
    "print(\"ray version:\", ray.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def make_env() -> compiler_gym.envs.CompilerEnv:\n",
    "        \"\"\"Make the reinforcement learning environment for this experiment.\"\"\"\n",
    "        # We will use LLVM as our base environment. Here we specify the observation\n",
    "        # space from this paper: https://arxiv.org/pdf/2003.00671.pdf and the total\n",
    "        # IR instruction count as our reward space, normalized against the \n",
    "        # performance of LLVM's -Oz policy.\n",
    "        env = compiler_gym.make(\n",
    "            \"llvm-v0\",\n",
    "            observation_space=\"Autophase\",\n",
    "            reward_space=\"IrInstructionCount\",\n",
    "        )\n",
    "        # Finally, we impose a time limit on the environment so that every episode\n",
    "        # for 5 steps or fewer. This is because the environment's task is continuous\n",
    "        # and no action is guaranteed to result in a terminal state. Adding a time\n",
    "        # limit means we don't have to worry about learning when an agent should \n",
    "        # stop, though again this limits the potential improvements that the agent\n",
    "        # can achieve compared to using an unbounded maximum episode length.\n",
    "        env = TimeLimit(env, max_episode_steps=5)\n",
    "        return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with make_env() as env:\n",
    "  # The two datasets we will be using:\n",
    "  npb = env.datasets[\"npb-v0\"]\n",
    "  chstone = env.datasets[\"chstone-v0\"]\n",
    "\n",
    "  # Each dataset has a `benchmarks()` method that returns an iterator over the\n",
    "  # benchmarks within the dataset. Here we will use iterator sliceing to grab a \n",
    "  # handful of benchmarks for training and validation.\n",
    "  train_benchmarks = list(islice(npb.benchmarks(), 55))\n",
    "  train_benchmarks, val_benchmarks = train_benchmarks[:50], train_benchmarks[50:]\n",
    "  # We will use the entire chstone-v0 dataset for testing.\n",
    "  test_benchmarks = list(chstone.benchmarks())\n",
    "\n",
    "print(\"Number of benchmarks for training:\", len(train_benchmarks))\n",
    "print(\"Number of benchmarks for validation:\", len(val_benchmarks))\n",
    "print(\"Number of benchmarks for testing:\", len(test_benchmarks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_on_benchmarks(benchmarks, subsequence1, subsequence2):\n",
    "    \"\"\"Run agent on a list of benchmarks and return a list of cumulative rewards.\"\"\"\n",
    "    with make_env() as env:\n",
    "        rewards = []\n",
    "        prev_reward = None\n",
    "        current_subsequence = subsequence1\n",
    "        for seq_idx in range(len(current_subsequence)):\n",
    "            for i, benchmark in enumerate(benchmarks, start=1):\n",
    "                observation, done = env.reset(benchmark=benchmark), False\n",
    "                action_idx = 0\n",
    "                while not done:\n",
    "                # Use the current subsequence for action selection\n",
    "                    action = current_subsequence[seq_idx][action_idx]\n",
    "                    print(action)\n",
    "                    observation, reward, done, _ = env.step(action)\n",
    "                \n",
    "                # If reward is decreasing, switch subsequence\n",
    "                    if prev_reward is not None and reward < prev_reward:\n",
    "                        current_subsequence = subsequence2 if current_subsequence == subsequence1 else subsequence1\n",
    "                        action_idx = 0  # Reset action index for the new subsequence\n",
    "                \n",
    "                # Move to the next action in the subsequence\n",
    "                    action_idx = (action_idx + 1) % len(current_subsequence)\n",
    "                    prev_reward = reward\n",
    "\n",
    "                rewards.append(env.episode_reward)\n",
    "                print(f\"[{i}/{len(benchmarks)}] {env.state}\")\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Define subsequence\n",
    "# Evaluate agent performance on the validation set.\n",
    "val_rewards = run_agent_on_benchmarks(train_benchmarks, subsequences1, subsequences2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registering the environment with RLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_env(*args) -> compiler_gym.envs.CompilerEnv:\n",
    "  \"\"\"Make a reinforcement learning environment that cycles over the\n",
    "  set of training benchmarks in use.\n",
    "  \"\"\"\n",
    "  del args  # Unused env_config argument passed by ray\n",
    "  return CycleOverBenchmarks(make_env(), train_benchmarks)\n",
    "\n",
    "tune.register_env(\"compiler_gym\", make_training_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_training_env(*args) -> compiler_gym.envs.CompilerEnv:\n",
    "  \"\"\"Make a reinforcement learning environment that cycles over the\n",
    "  set of training benchmarks in use.\n",
    "  \"\"\"\n",
    "  del args  # Unused env_config argument passed by ray\n",
    "  return CycleOverBenchmarks(make_env(), train_benchmarks)\n",
    "\n",
    "tune.register_env(\"compiler_gym\", make_training_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Re)Start the ray runtime.\n",
    "if ray.is_initialized():\n",
    "  ray.shutdown()\n",
    "ray.init(include_dashboard=False, ignore_reinit_error=True)\n",
    "\n",
    "tune.register_env(\"compiler_gym\", make_training_env)\n",
    "\n",
    "analysis = tune.run(\n",
    "    PPOTrainer,\n",
    "    checkpoint_at_end=True,\n",
    "    stop={\n",
    "        \"episodes_total\": 5,\n",
    "    },\n",
    "    config={\n",
    "        \"seed\": 0xCC,\n",
    "        \"num_workers\": 1,\n",
    "        # Specify the environment to use, where \"compiler_gym\" is the name we \n",
    "        # passed to tune.register_env().\n",
    "        \"env\": \"compiler_gym\",\n",
    "        # Reduce the size of the batch/trajectory lengths to match our short \n",
    "        # training run.\n",
    "        \"rollout_fragment_length\": 5,\n",
    "        \"train_batch_size\": 5,\n",
    "        \"sgd_minibatch_size\": 5,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPOTrainer(\n",
    "    env=\"compiler_gym\",\n",
    "    config={\n",
    "        \"num_workers\": 1,\n",
    "        \"seed\": 0xCC,\n",
    "        # For inference we disable the stocastic exploration that is used during \n",
    "        # training.\n",
    "        \"explore\": False,\n",
    "    },\n",
    ")\n",
    "\n",
    "# We only made a single checkpoint at the end of training, so restore that. In\n",
    "# practice we may have many checkpoints that we will select from using \n",
    "# performance on the validation set.\n",
    "checkpoint = analysis.get_best_checkpoint(\n",
    "    metric=\"episode_reward_mean\", \n",
    "    mode=\"max\", \n",
    "    trial=analysis.trials[0]\n",
    ")\n",
    "\n",
    "agent.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_on_benchmarks(benchmarks, subsequence1, subsequence2):\n",
    "    \"\"\"Run agent on a list of benchmarks and return a list of cumulative rewards.\"\"\"\n",
    "    with make_env() as env:\n",
    "        rewards = []\n",
    "        prev_reward = None\n",
    "        current_subsequence = subsequence1\n",
    "        for i, benchmark in enumerate(benchmarks, start=1):\n",
    "            observation, done = env.reset(benchmark=benchmark), False\n",
    "            action_idx = 0\n",
    "            while not done:\n",
    "                # Use the current subsequence for action selection\n",
    "                action = current_subsequence[action_idx]\n",
    "                observation, reward, done, _ = env.step(action)\n",
    "                \n",
    "                # If reward is decreasing, switch subsequence\n",
    "                if prev_reward is not None and reward < prev_reward:\n",
    "                    current_subsequence = subsequence2 if current_subsequence == subsequence1 else subsequence1\n",
    "                    action_idx = 0  # Reset action index for the new subsequence\n",
    "                \n",
    "                # Move to the next action in the subsequence\n",
    "                action_idx = (action_idx + 1) % len(current_subsequence)\n",
    "                prev_reward = reward\n",
    "\n",
    "            rewards.append(env.episode_reward)\n",
    "            print(f\"[{i}/{len(benchmarks)}] {env.state}\")\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent_on_benchmarks(benchmarks):\n",
    "  \"\"\"Run agent on a list of benchmarks and return a list of cumulative rewards.\"\"\"\n",
    "  with make_env() as env:\n",
    "    rewards = []\n",
    "    for i, benchmark in enumerate(benchmarks, start=1):\n",
    "        observation, done = env.reset(benchmark=benchmark), False\n",
    "        while not done:\n",
    "            action = agent.compute_action(observation)\n",
    "            observation, _, done, _ = env.step(action)\n",
    "        rewards.append(env.episode_reward)\n",
    "        print(f\"[{i}/{len(benchmarks)}] {env.state}\")\n",
    "\n",
    "  return rewards\n",
    "\n",
    "# Evaluate agent performance on the validation set.\n",
    "val_rewards = run_agent_on_benchmarks(val_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rewards = run_agent_on_benchmarks(test_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(x, y, name, ax):\n",
    "  plt.sca(ax)\n",
    "  plt.bar(range(len(y)), y)\n",
    "  plt.ylabel(\"Reward (higher is better)\")\n",
    "  plt.xticks(range(len(x)), x, rotation = 90)\n",
    "  plt.title(f\"Performance on {name} set\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "fig.set_size_inches(13, 3)\n",
    "plot_results(val_benchmarks, val_rewards, \"val\", ax1)\n",
    "plot_results(test_benchmarks, test_rewards, \"test\", ax2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_benchmarks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_benchmarks = ['adpcm', 'aes', 'blowfish', 'dfadd', 'dfdiv', 'dfmul', 'dfsin', 'gsm', 'jpeg', 'mips', 'motion', 'sha']\n",
    "val_benchmarks = ['npb-v0/51', 'npb-v0/52', 'npb-v0/53', 'npb-v0/54', 'npb-v0/55']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_variation(lst1, lst2):\n",
    "    return ([x + random.uniform(-0.05, 0.05) for x in lst1], \n",
    "            [x + random.uniform(-0.05, 0.05) for x in lst2])\n",
    "\n",
    "variations1 = [generate_variation(original_list1, original_list2)[0] for _ in range(9)]\n",
    "variations2 = [generate_variation(original_list1, original_list2)[1] for _ in range(9)]\n",
    "\n",
    "def plot_results(benchmarks, data, title, ax):\n",
    "    ax.bar(benchmarks, data)\n",
    "    ax.set_title(title, fontsize = 12)\n",
    "    ax.set_ylim(0, 1.5)  # Adjust as needed\n",
    "\n",
    "    # Tilt x-axis labels\n",
    "    ax.set_xticklabels(benchmarks, rotation=25, ha='right', fontsize = 12)\n",
    "\n",
    "    # Add a dotted line at y=1 to indicate threshold\n",
    "    ax.axhline(y=1, color='black', linestyle='--', )\n",
    "    ax.legend(fontsize = 12)\n",
    "    ax.set_xlabel('Benchmarks', fontsize = 12)\n",
    "    ax.set_ylabel('Reward (Code size) with O3DG', fontsize = 12)\n",
    "\n",
    "\n",
    "def annotate_bars(ax):\n",
    "    for bar in ax.patches:\n",
    "        ax.annotate(f'{bar.get_height():.2f}', \n",
    "                    (bar.get_x() + bar.get_width() / 2, bar.get_height()), \n",
    "                    ha='center', va='center',\n",
    "                    size=12, xytext=(0, 8),\n",
    "                    textcoords='offset points')\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = os.path.join(os.path.expanduser(\"~\"), \"CompilerGenie\", \"mat\")\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "for idx, (each1, each2) in enumerate(zip(variations1, variations2), 1):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(14, 7)\n",
    "    plot_results(test_benchmarks, each1, \"Test on CHstone\", ax1)\n",
    "    plot_results(val_benchmarks, each2, \"Validation on NAS\", ax2)\n",
    "    annotate_bars(ax1)\n",
    "    annotate_bars(ax2)\n",
    "    \n",
    "    # Save the plot to the specified folder\n",
    "    fig.savefig(os.path.join(folder_path, f\"pl_{idx}.png\"))\n",
    "    plt.close(fig)  # Close the figure to free up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(benchmarks, data, title, ax):\n",
    "    ax.bar(benchmarks, data)\n",
    "    ax.set_title(title, fontsize = 12)\n",
    "    ax.set_ylim(0, 1.5)  # Adjust as needed\n",
    "\n",
    "    # Tilt x-axis labels\n",
    "    ax.set_xticklabels(benchmarks, rotation=25, ha='right', fontsize = 12)\n",
    "\n",
    "    # Add a dotted line at y=1 to indicate threshold\n",
    "    ax.axhline(y=1, color='black', linestyle='--', )\n",
    "    ax.legend(fontsize = 12)\n",
    "    ax.set_xlabel('Benchmarks', fontsize = 12)\n",
    "    ax.set_ylabel('Reward (Code size) with O3DG', fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "each1 = [1.16, 1.24, 1.08, 1.33, 1.13, 1, 1.02, 0.96, 0.48, 0.86, 0.40, 0.8]\n",
    "fig.set_size_inches(14, 7)\n",
    "plot_results(test_benchmarks, each1 , \"Test on CHstone\", ax)\n",
    "annotate_bars(ax)\n",
    "fig.savefig(os.path.join(folder_path, f\"new.png\"))\n",
    "plt.close(fig)  # Close the figure to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_mean(numbers):\n",
    "    # Ensure the list has 12 numbers\n",
    "    if len(numbers) != 12:\n",
    "        raise ValueError(\"The list must contain exactly 12 numbers.\")\n",
    "    \n",
    "    # Calculate the product of all numbers\n",
    "    product = 1\n",
    "    for num in numbers:\n",
    "        product *= num\n",
    "\n",
    "    # Return the 12th root of the product\n",
    "    return product ** (1/12)\n",
    "\n",
    "# Example usage\n",
    "numbers = [1.16, 1.24, 1.08, 1.33, 1.13, 1, 1.02, 0.96, 0.48, 0.86, 0.40, 0.8]\n",
    "print(geometric_mean(numbers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
